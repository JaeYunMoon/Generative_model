{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sim2real\\AppData\\Local\\miniconda3\\envs\\fish\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os,glob\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time \n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from  torchinfo import summary \n",
    "import torchsummary\n",
    "import time\n",
    "\n",
    "from PIL import Image \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET \n",
    "- 데이터셋 다운로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !wget https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/cityscapes.tar.gz -O cityscapes.tar.gz\n",
    "# !tar -zxvf facades.tar.gz -C ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 A와 B의 개수: 2975\n",
      "평가 데이터셋 A와 B의 개수: 500\n",
      "이미지 크기: (512, 256)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/cityscapes/train/1.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이미지 크기:\u001b[39m\u001b[38;5;124m\"\u001b[39m, image\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(image)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"학습 데이터셋 A와 B의 개수:\", len(next(os.walk('./dataset/cityscapes/train/'))[2]))\n",
    "print(\"평가 데이터셋 A와 B의 개수:\", len(next(os.walk('./dataset/cityscapes/val/'))[2]))\n",
    "#print(\"테스트 데이터셋 A와 B의 개수:\", len(next(os.walk('./data/cityscapes/test/'))[2]))\n",
    "# 한 쌍의 이미지 출력(왼쪽은 정답 이미지, 오른쪽은 조건 이미지)\n",
    "image = Image.open('./dataset/cityscapes/train/1.jpg')\n",
    "print(\"이미지 크기:\", image.size)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,root,transforms_=None,mode=\"train\"):\n",
    "        self.transform = transforms_\n",
    "        self.files = sorted(glob.glob(os.path.join(root,mode)+\"/*.jpg\"))\n",
    "        if mode == \"train\":\n",
    "            self.files.extend(sorted(glob.glob(os.path.join(root,\"test\")+\"/*.jpg\")))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index%len(self.files)])\n",
    "        w,h = img.size\n",
    "        img_A = img.crop((0,0,w/2,h)) # 이미지의 왼쪽 절반 \n",
    "        img_B = img.crop((w/2,0,w,h)) # 이미지의 오른쪽 절반 \n",
    "\n",
    "        # 데이터 증강(Data Agumentation)을 위한 좌우 반전(horizontal flip)\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:,::-1,:],\"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:,::-1,:],\"RGB\")\n",
    "        \n",
    "        img_A = self.transform(img_A)\n",
    "        img_B = self.transform(img_B)\n",
    "\n",
    "        return {\"A\":img_A,\"B\":img_B}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sim2real\\AppData\\Local\\Temp\\ipykernel_30608\\3409668146.py:2: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  transforms.Resize((256,256),Image.BICUBIC),\n",
      "c:\\Users\\sim2real\\AppData\\Local\\miniconda3\\envs\\fish\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize((256,256),Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,.5,.5))\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(\"./dataset/cityscapes\",transforms_=transforms_)\n",
    "val_dataset = ImageDataset(\"./dataset/cityscapes\",transforms_=transforms_)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=10,shuffle=True,num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=10,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net 아키텍처의 다운 샘플링(Down Sampling) 모듈\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        # 너비와 높이가 2배씩 감소\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "# U-Net 아키텍처의 업 샘플링(Up Sampling) 모듈: Skip Connection 사용\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        # 너비와 높이가 2배씩 증가\n",
    "        layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1) # 채널 레벨에서 합치기(concatenation)\n",
    "\n",
    "        return x\n",
    "# U-Net 생성자(Generator) 아키텍처\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False) # 출력: [64 X 128 X 128]\n",
    "        self.down2 = UNetDown(64, 128) # 출력: [128 X 64 X 64]\n",
    "        self.down3 = UNetDown(128, 256) # 출력: [256 X 32 X 32]\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5) # 출력: [512 X 16 X 16]\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 8 X 8]\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 4 X 4]\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 2 X 2]\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5) # 출력: [512 X 1 X 1]\n",
    "\n",
    "        # Skip Connection 사용(출력 채널의 크기 X 2 == 다음 입력 채널의 크기)\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5) # 출력: [1024 X 2 X 2]\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 4 X 4]\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 8 X 8]\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 16 X 16]\n",
    "        self.up5 = UNetUp(1024, 256) # 출력: [512 X 32 X 32]\n",
    "        self.up6 = UNetUp(512, 128) # 출력: [256 X 64 X 64]\n",
    "        self.up7 = UNetUp(256, 64) # 출력: [128 X 128 X 128]\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2), # 출력: [128 X 256 X 256]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, kernel_size=4, padding=1), # 출력: [3 X 256 X 256]\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # 인코더부터 디코더까지 순전파하는 U-Net 생성자(Generator)\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net 판별자(Discriminator) 아키텍처\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_channels, out_channels, normalization=True):\n",
    "            # 너비와 높이가 2배씩 감소\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 두 개의 이미지(실제/변환된 이미지, 조건 이미지)를 입력 받으므로 입력 채널의 크기는 2배\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False), # 출력: [64 X 128 X 128]\n",
    "            *discriminator_block(64, 128), # 출력: [128 X 64 X 64]\n",
    "            *discriminator_block(128, 256), # 출력: [256 X 32 X 32]\n",
    "            *discriminator_block(256, 512), # 출력: [512 X 16 X 16]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1, bias=False) # 출력: [1 X 16 X 16]\n",
    "        )\n",
    "\n",
    "    # img_A: 실제/변환된 이미지, img_B: 조건(condition)\n",
    "    def forward(self, img_A, img_B):\n",
    "        # 이미지 두 개를 채널 레벨에서 연결하여(concatenate) 입력 데이터 생성\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# 생성자(generator)와 판별자(discriminator) 초기화\n",
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "\n",
    "# 가중치(weights) 초기화\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# 손실 함수(loss function)\n",
    "# criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_GAN = torch.nn.BCEWithLogitsLoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "criterion_GAN.cuda()\n",
    "criterion_pixelwise.cuda()\n",
    "\n",
    "# 학습률(learning rate) 설정\n",
    "lr = 0.0002\n",
    "\n",
    "# 생성자와 판별자를 위한 최적화 함수\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([10, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 200 # 학습의 횟수(epoch) 설정\n",
    "sample_interval = 200 # 몇 번의 배치(batch)마다 결과를 출력할 것인지 설정\n",
    "\n",
    "# 변환된 이미지와 정답 이미지 사이의 L1 픽셀 단위(pixel-wise) 손실 가중치(weight) 파라미터\n",
    "lambda_pixel = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        # 모델의 입력(input) 데이터 불러오기\n",
    "        real_A = batch[\"B\"].cuda()\n",
    "        real_B = batch[\"A\"].cuda()\n",
    "\n",
    "        # 진짜(real) 이미지와 가짜(fake) 이미지에 대한 정답 레이블 생성 (너바와 높이를 16씩 나눈 크기)\n",
    "        real = torch.cuda.FloatTensor(real_A.size(0), 1, 16, 16).fill_(1.0) # 진짜(real): 1\n",
    "        fake = torch.cuda.FloatTensor(real_A.size(0), 1, 16, 16).fill_(0.0) # 가짜(fake): 0\n",
    "\n",
    "        \"\"\" 생성자(generator)를 학습합니다. \"\"\"\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # 이미지 생성\n",
    "        fake_B = generator(real_A)\n",
    "        # print(fake_B.size())\n",
    "\n",
    "        # 생성자(generator)의 손실(loss) 값 계산\n",
    "        loss_GAN = criterion_GAN(discriminator(fake_B, real_A), real)\n",
    "\n",
    "        # 픽셀 단위(pixel-wise) L1 손실 값 계산\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B) \n",
    "\n",
    "        # 최종적인 손실(loss)\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        # 생성자(generator) 업데이트\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \"\"\" 판별자(discriminator)를 학습합니다. \"\"\"\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # 판별자(discriminator)의 손실(loss) 값 계산\n",
    "        loss_real = criterion_GAN(discriminator(real_B, real_A), real) # 조건(condition): real_A\n",
    "        loss_fake = criterion_GAN(discriminator(fake_B.detach(), real_A), fake)\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # 판별자(discriminator) 업데이트\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        done = epoch * len(train_dataloader) + i\n",
    "\n",
    "        if done % sample_interval == 0:\n",
    "            imgs = next(iter(val_dataloader)) # 10개의 이미지를 추출해 생성\n",
    "            real_A = imgs[\"B\"].cuda()\n",
    "            real_B = imgs[\"A\"].cuda()\n",
    "            fake_B = generator(real_A)\n",
    "            # real_A: 조건(condition), fake_B: 변환된 이미지(translated image), real_B: 정답 이미지\n",
    "            img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "            save_image(img_sample, f\"{done}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # 하나의 epoch이 끝날 때마다 로그(log) 출력\n",
    "    print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {loss_D.item():.6f}] [G pixel loss: {loss_pixel.item():.6f}, adv loss: {loss_GAN.item()}] [Elapsed time: {time.time() - start_time:.2f}s]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
